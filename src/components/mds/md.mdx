# Framework

![](./pipeline3.png)

**Data construction and training pipeline of GeoAgent.** GeoSeek-CoT contains 10k high-quality reasoning processes labeled by geography experts and geolocation game players. 
  GeoSeek-Loc includes 20k images for the cold start **of GeoAgent-SFT.** During the GRPO-based training, based on **GeoAgent-SFT**, we design the geo-similarity reward to encourage the model to converge towards correct answers both physically and semantically. 
  Also, the consistency reward is introduced to keep the integrity and consistency of CoT.

# GeoSeek Dataset

![](./teaser.png)

**GeoSeek Dataset.** We train GeoAgent with GeoSeek, a geolocation dataset with bias-reducing sampling and a val-bench annotated with locatability and geographic elements. Remarkably, a single image may contain multiple geographic elements.

<img src="./data.png" style={{ display: "block", width: "80%", maxWidth: "80%", margin: "0 auto" }} />

GeoSeek consists of three parts: GeoSeek-CoT, GeoSeek-Loc, and GeoSeek-Val. GeoSeek-CoT contains 10K high-quality reasoning processes labeled by humans, used for SFT. GeoSeek-Loc contains 20K samples with fine-grained address annotations, used for GRPO. GeoSeek-Val serves as a benchmark for evaluating geolocation models, with each image labeled for locatability and the geographic clues available for localization.

# Comparison

![](./ccase.png)

**Reasoning comparison of six different models (GPT-5, Gemma3, Kimi, Qwen2.5-VL-32B, GeoAgent, and GeoAgent with only SFT).**

- **Green**: correct reasoning  
- **Yellow**: reasoning with certain issues  
- **Red**: reasoning that deviates significantly  
- **Brown**: highlights instances where the reasoning process is **incomplete** or **inconsistent** with the result.

# Acknowledgments

We sincerely thank
[Yue Zhang](https://tuxun.fun/),
[Mu He](https://space.bilibili.com/1655209518?spm_id_from=333.337.0.0),
[Haowen He](https://space.bilibili.com/111714204?spm_id_from=333.337.0.0),
[Jie Lin](https://space.bilibili.com/93569847?spm_id_from=333.337.0.0),
and other experts in geography, as well as outstanding geolocation game players,
for their valuable guidance, prompt design suggestions, and data support throughout the construction of the GeoSeek dataset.

We also thank 
[Zhixiang Wang](https://tuxun.fun/), 
[Chilin Chen](https://tuxun.fun/), 
[Jincheng Shi](https://tuxun.fun/), 
[Liupeng Zhang](https://tuxun.fun/), 
[Yuan Gu](https://tuxun.fun/), 
[Yanghang Shao](https://tuxun.fun/), 
[Jinhua Zhang](https://tuxun.fun/), 
[Jiachen Zhu](https://tuxun.fun/), 
[Gucheng Qiuyue](https://tuxun.fun/), 
[Qingyang Guo](https://tuxun.fun/), 
[Jingchen Yang](https://tuxun.fun/), 
[Weilong Kong](https://tuxun.fun/), 
[Xinyuan Li](https://tuxun.fun/), 
and [Mr. Xu](https://tuxun.fun/) (an anonymous volunteer) 
for their outstanding contributions in providing high-quality reasoning process data.

# License

<div class="license-content">
  <div class="license-container">
    <div class="license-icon">
      <i class="fab fa-creative-commons"></i>
      <i class="fab fa-creative-commons-by"></i>
      <i class="fab fa-creative-commons-nc"></i>
    </div>
    <div class="license-text">
      <h3 data-en="Creative Commons Attribution-NonCommercial 4.0 International" data-zh="Áü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî® 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆ">Creative Commons Attribution-NonCommercial 4.0 International</h3>
      <p data-en="This code is licensed under the Creative Commons Attribution-NonCommercial 4.0 International for non-commercial use only. Please note that any commercial use of this code requires formal permission prior to use." data-zh="Ê≠§‰ª£Á†ÅÊ†πÊçÆÁü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî® 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆÊéàÊùÉÔºå‰ªÖ‰æõÈùûÂïÜ‰∏öÁî®ÈÄî‰ΩøÁî®„ÄÇËØ∑Ê≥®ÊÑèÔºå‰ªª‰ΩïÂïÜ‰∏öÁî®ÈÄîÈÉΩÈúÄË¶Å‰∫ãÂÖàËé∑ÂæóÊ≠£ÂºèËÆ∏ÂèØ„ÄÇ">This code is licensed under the Creative Commons Attribution-NonCommercial 4.0 International for non-commercial use only. Please note that any commercial use of this code requires formal permission prior to use.</p>
      <div class="license-actions">
        <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener noreferrer" class="license-btn">
          <i class="fas fa-external-link-alt"></i>
          <span data-en="View License" data-zh="Êü•ÁúãËÆ∏ÂèØËØÅ">View License</span>
        </a>
      </div>
    </div>
  </div>
</div>


# Contact

### Feel free to contact us at: jin_modi[AT]mail.nankai.edu.cn

### For commercial licensing, please contact: andrewhoux[AT]gmail.com



{/* ### Nice (H3)

[**Diffusion models**](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) are a class of generative models in machine learning that have gained significant attention in recent years. 

**Text-to-image diffusion models** have various application scenarios, including:

- **Art & Design**: Used by artists for creative art and fashion designers to generate clothing renderings.
- **Advertising & Marketing**: Helps in creating advertising posters and product promotion maps.
- **Game & Film**: Applied in game scene and character design, as well as film and television concept design...

> Thegeneral training steps of diffusion models are as follows:

1. Data collection
2. Noise addition
3. Model building
4. Training
5. Evaluation
6. Deployment


```
import torch 

def denoise():
    pass
```

* [ ] to do
* [x] done
* [x] ~114514~

![](./logo_vision_aigc.png)

Lift ($$L$$) can be determined by Lift Coefficient ($$C_L$$) like the following
equation.

$$
L = \frac{1}{2} \rho v^2 S C_L
$$


### Table (H3)

| a       | b       | c     |  d      |
| -       | :-      | -:    | :-:     |
| Alpha   | Bravo   | Alpha | Bravo   |
| ‰∏≠Êñá    | Charlie | üë©‚Äç‚ù§Ô∏è‚Äçüë©    | Delta   | */}